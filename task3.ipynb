{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import *\n",
    "import numpy as np\n",
    "with open('task3_train.txt','r')as f:\n",
    "    task3_train=f.read()\n",
    "task3_train=JSONDecoder().decode(task3_train)\n",
    "\n",
    "with open('task3_hash.txt','r')as f:\n",
    "    task3_hash=f.read()\n",
    "task3_hash=JSONDecoder().decode(task3_hash)\n",
    "\n",
    "with open('task3_trainB.txt','r')as f:\n",
    "    task3_trainB=f.read()\n",
    "task3_trainB=JSONDecoder().decode(task3_trainB)\n",
    "\n",
    "with open('task3_test.txt','r')as f:\n",
    "    task3_test=f.read()\n",
    "task3_test=JSONDecoder().decode(task3_test)\n",
    "\n",
    "with open('task3_train_feature.txt','r')as f:\n",
    "    train_feature=f.read()\n",
    "train_feature=JSONDecoder().decode(train_feature)\n",
    "\n",
    "with open('task3_test_feature.txt','r')as f:\n",
    "    test_feature=f.read()\n",
    "test_feature=JSONDecoder().decode(test_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':[0,9999],'UNK':[1,9999]}\n",
    "pos_dict={'PADDING':0,'UNK':1}\n",
    "\n",
    "for sent in task3_train:\n",
    "    for word in sent['word']:\n",
    "        if not str(word) in word_dict:\n",
    "            word_dict[str(word)]=[len(word_dict),1]\n",
    "        else:\n",
    "            word_dict[str(word)][1]+=1\n",
    "    for pos in sent['pos']:\n",
    "        if not pos in pos_dict:\n",
    "            pos_dict[str(pos)]=len(pos_dict)\n",
    "            \n",
    "for sent in task3_hash:\n",
    "    for word in sent['word']:\n",
    "        if not  str(word) in word_dict:\n",
    "            word_dict[str(word)]=[len(word_dict),1]\n",
    "        else:\n",
    "            word_dict[str(word)][1]+=1\n",
    "    for pos in sent['pos']:\n",
    "        if not pos in pos_dict:\n",
    "            pos_dict[str(pos)]=len(pos_dict)\n",
    "            \n",
    "            \n",
    "for sent in task3_test:\n",
    "    for word in sent['word']:\n",
    "        if not  str(word) in word_dict:\n",
    "            word_dict[str(word)]=[len(word_dict),1]\n",
    "        else:\n",
    "            word_dict[str(word)][1]+=1\n",
    "    for pos in sent['pos']:\n",
    "        if not pos in pos_dict:\n",
    "            pos_dict[str(pos)]=len(pos_dict)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdict=dict()\n",
    "\n",
    "with open('/home/wuch/word2vec_twitter_model.bin','rb')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1).decode(errors='ignore')\n",
    "            if ch ==' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if len(word) != 0:\n",
    "            tp= np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            if word in word_dict:\n",
    "                embdict[str(word)]=tp.tolist()\n",
    "\n",
    "        else:\n",
    "            f.read(binary_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdict2=dict()\n",
    "\n",
    "#model_swm_300-6-10-low.w2v\n",
    "with open('/data1/wuch/model_swm_300-6-10-low.w2v','r',encoding='utf-8',errors='ignore')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        k = f.readline().split()\n",
    "        word=k[0]\n",
    "        if len(word) != 0:\n",
    "            tp=[float(x) for x in k[1:]]\n",
    "            #tp= np.fromstring(' '.join(k[1:]), dtype='float32')\n",
    "            if word in word_dict:\n",
    "                embdict2[str(word)]=tp\n",
    "        else:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import cholesky\n",
    "print(len(embdict),len(word_dict))\n",
    "print(len(word_dict))\n",
    "lister=[0]*len(word_dict)\n",
    "xp=np.zeros(400,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict.keys():\n",
    "    lister[word_dict[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(lister[word_dict[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(lister)):\n",
    "    if type(lister[i])==int:\n",
    "        lister[i]=np.reshape(norm, 400)\n",
    "lister[0]=np.zeros(400,dtype='float32')\n",
    "lister=np.array(lister,dtype='float32')\n",
    "print(lister.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import cholesky\n",
    "print(len(embdict2),len(word_dict))\n",
    "print(len(word_dict))\n",
    "lister2=[0]*len(word_dict)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict2.keys():\n",
    "    lister2[word_dict[i][0]]=np.array(embdict2[i],dtype='float32')\n",
    "    cand.append(lister2[word_dict[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(lister2)):\n",
    "    if type(lister2[i])==int:\n",
    "        lister2[i]=np.reshape(norm, 300)\n",
    "lister2[0]=np.zeros(300,dtype='float32')\n",
    "lister2=np.array(lister2,dtype='float32')\n",
    "print(lister2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_num=[1923.,1390.,316.,205.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word=[]\n",
    "train_pos=[]\n",
    "train_class=[]\n",
    "train_label=[]\n",
    "train_weight=[]\n",
    "train_index=[]\n",
    "dev_word=[]\n",
    "dev_pos=[]\n",
    "dev_class=[]\n",
    "dev_label=[]\n",
    "\n",
    "test_word=[]\n",
    "test_pos=[]\n",
    "test_class=[]\n",
    "test_label=[]\n",
    "min_freq=2\n",
    "maxlen=50\n",
    "\n",
    "for sent in task3_hash:\n",
    "\n",
    "    \n",
    "    thash=[]\n",
    "    for j in range(len(sent['word'])):\n",
    "        if sent['word'][j] in embdict  or str(sent['word'][j]) in embdict2:\n",
    "          \n",
    "            if str(sent['word'][j])=='#irony':\n",
    "                thash=[1,0,0,0]\n",
    "            elif str(sent['word'][j])=='#not':\n",
    "                thash=[0,1,0,0]\n",
    "            elif str(sent['word'][j])=='#sarcasm' or str(sent['word'][j])=='#sarcastic':\n",
    "                thash=[0,0,1,0]\n",
    "            else:\n",
    "                thash=[0,0,0,1]\n",
    "    train_index.append(thash)\n",
    "weight=[1.0,1.2,1.5,2.0] \n",
    "for sent in task3_train:  \n",
    "    train_label.append(float(sent['label']))\n",
    "for sent in task3_trainB:  \n",
    "    tw=[]\n",
    "    tp=[]\n",
    "    for j in range(len(sent['word'])):\n",
    "        if sent['word'][j] in embdict  or str(sent['word'][j]) in embdict2:\n",
    "            \n",
    "            if str(sent['word'][j])=='USERNAME':\n",
    "                \n",
    "                tw.append(word_dict['USERNAME'][0])\n",
    "                poss=[0]*len(pos_dict)\n",
    "                poss[pos_dict['@']]=1\n",
    "                tp.append(poss)\n",
    "            \n",
    "            else:\n",
    "                tw.append(word_dict[str(sent['word'][j])][0])\n",
    "                poss=[0]*len(pos_dict)\n",
    "                poss[pos_dict[str(sent['pos'][j])]]=1\n",
    "                tp.append(poss)\n",
    "    train_weight.append(np.sqrt(weight_num[0]/weight_num[int(sent['label'])]))\n",
    "    if len(tw)>maxlen:\n",
    "        maxlen=len(tw)\n",
    "    tw=tw+[0]*(maxlen-len(tw))\n",
    "    poslist=[0]*len(pos_dict)\n",
    "    tp=tp+[poslist]*(maxlen-len(tp))\n",
    "    train_word.append(tw)\n",
    "    \n",
    "    train_pos.append(tp)\n",
    "    #train_label.append(float(sent['label']))\n",
    "train_labelB=[]\n",
    "for sent in task3_trainB:\n",
    "    if int(sent['label']==0):\n",
    "        train_labelB.append([1,0,0,0])\n",
    "    if int(sent['label']==1):\n",
    "        train_labelB.append([0,1,0,0])\n",
    "    if int(sent['label']==2):\n",
    "        train_labelB.append([0,0,1,0])\n",
    "    if int(sent['label']==3):\n",
    "        train_labelB.append([0,0,0,1])\n",
    "for sent in task3_test:\n",
    "    tw=[]\n",
    "    tp=[]\n",
    "\n",
    "    for j in range(len(sent['word'])):\n",
    "        if sent['word'][j] in embdict  or str(sent['word'][j]) in embdict2:\n",
    "            if str(sent['word'][j])=='USERNAME':\n",
    "                \n",
    "                tw.append(word_dict['USERNAME'][0])\n",
    "                poss=[0]*len(pos_dict)\n",
    "                poss[pos_dict['@']]=1\n",
    "                tp.append(poss)\n",
    "            \n",
    "            else:\n",
    "                tw.append(word_dict[str(sent['word'][j])][0])\n",
    "                poss=[0]*len(pos_dict)\n",
    "                poss[pos_dict[str(sent['pos'][j])]]=1\n",
    "                tp.append(poss)\n",
    "\n",
    "    if len(tw)>maxlen:\n",
    "        maxlen=len(tw)\n",
    "    tw=tw+[0]*(maxlen-len(tw))\n",
    "    poslist=[0]*len(pos_dict)\n",
    "    tp=tp+[poslist]*(maxlen-len(tp))\n",
    "    test_word.append(tw)\n",
    "    test_pos.append(tp)\n",
    "    test_label.append(float(sent['label']))\n",
    "print(maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FP = \"./SemEval2018-T3_gold_test_taskB_emoji.txt\"\n",
    "\n",
    "\n",
    "corpus3, y3 = parse_dataset(DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf  \n",
    "import keras\n",
    "from keras import activations \n",
    "from keras import initializers \n",
    "from keras import constraints \n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import GRU,Bidirectional,Reshape,Multiply,Conv1D,MaxPooling1D,GlobalMaxPooling1D,GlobalAveragePooling1D,concatenate\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout,multiply, Input,Activation,wrappers,Merge,merge,core,advanced_activations,Conv2D,MaxPooling2D\n",
    "from keras.layers import LSTM,Average,average,BatchNormalization\n",
    "from keras.layers import Masking, Embedding,Flatten,Multiply,RepeatVector,Permute,TimeDistributed,add,GlobalAveragePooling2D\n",
    "from keras.regularizers import l2,l1\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "all_score=[]\n",
    "all_pred=[]\n",
    "for fold in range(1):\n",
    "    \n",
    "    test_word=np.array(test_word,dtype='float32')\n",
    "    test_pos=np.array(test_pos,dtype='float32')\n",
    "    test_feature=np.array(test_feature,dtype='float32')\n",
    "    \n",
    "    split_train_word=np.array(train_word)\n",
    "    split_train_pos=np.array(train_pos)\n",
    "    split_train_feature=np.array(train_feature)\n",
    "    split_train_label=np.array(train_label)\n",
    "    split_train_labelB=np.array(train_labelB)\n",
    "    split_train_index=np.array(train_index)\n",
    "    split_train_weight=np.array(train_weight)\n",
    "\n",
    "    ensen=10\n",
    "    for wei in range(ensen):\n",
    "        \n",
    "        \n",
    "        sequenceLength =50\n",
    "        fmin=0\n",
    "        batch_size=64\n",
    "\n",
    "        x_train = Input(batch_shape=(None,sequenceLength),dtype='float32')\n",
    "        x_train1 = Input(batch_shape=(None,sequenceLength,len(pos_dict)),dtype='float32')\n",
    "        x_train3 = Input(batch_shape=(None,149),dtype='float32')\n",
    "        emb_drop=0.2\n",
    "        hid1=500\n",
    "        \n",
    "        e0=Embedding(len(word_dict), 400,weights=[lister],input_length=sequenceLength,mask_zero=True)(x_train)\n",
    "        d0=Dropout(emb_drop)(e0)\n",
    "        e1=Embedding(len(word_dict), 300,weights=[lister2],input_length=sequenceLength,mask_zero=True)(x_train)\n",
    "        d1=Dropout(emb_drop)(e1)\n",
    "        #e1=Embedding(100, len(pos_dict),input_length=sequenceLength,mask_zero=True)(x_train1)\n",
    "        #d1=Dropout(emb_drop)(e1)\n",
    "        \n",
    "        h_merge1=merge([d0,d1,x_train1], mode='concat',concat_axis=2)\n",
    "        #fa=Reshape((sequenceLength,400+len(pos_dict),1))(h_merge1)\n",
    "        flat_beta=Bidirectional(GRU(300,return_sequences=True,dropout=0.2,recurrent_dropout=0.2))(h_merge1)\n",
    "        flat_beta2=Bidirectional(GRU(300,return_sequences=True,dropout=0.2,recurrent_dropout=0.2))(flat_beta)\n",
    "        h_merge3=concatenate([flat_beta,flat_beta2])\n",
    "        #\n",
    "        flat_beta3=Bidirectional(GRU(300,return_sequences=False,dropout=0.4,recurrent_dropout=0.4))(h_merge3)\n",
    "        \n",
    "        cr2=Dense(200,activation='relu')(flat_beta3)\n",
    "        cr4=Dense(4,activation='softmax')(cr2)\n",
    "        \n",
    "    \n",
    "        \n",
    "        h_merge6=concatenate([cr2,cr4,flat_beta3])\n",
    "        cr1=Dense(200,activation='relu')(h_merge6)\n",
    "        \n",
    "        cr3=Dense(4,activation='softmax',name='taskb')(cr1)\n",
    "        \n",
    "        Acr1=Dense(200,activation='relu')(h_merge6)\n",
    "        \n",
    "        Acr3=Dense(1,activation='sigmoid')(Acr1)\n",
    "        \n",
    "        model=Model([x_train,x_train1,x_train3],[cr3,cr4,Acr3])\n",
    "        \n",
    "        model.compile('rmsprop', loss=['categorical_crossentropy','categorical_crossentropy','binary_crossentropy'],\n",
    "                      metrics=['acc','acc'],loss_weights=[0.5,0.5,1.])\n",
    "        \n",
    "        for _ in range(3):\n",
    "            model.fit([split_train_word,split_train_pos,split_train_feature],[split_train_labelB,split_train_index,split_train_label],\n",
    "                      batch_size=batch_size, nb_epoch=1,shuffle=True,sample_weight={'taskb':split_train_weight})\n",
    "\n",
    "\n",
    "            classes=np.argmax(model.predict([test_word,test_pos,test_feature])[0],axis=1)\n",
    "            classes2=np.round(model.predict([test_word,test_pos,test_feature])[2])\n",
    "            score = sklearn.metrics.f1_score(y2, classes2, pos_label=1)\n",
    "            score2 = sklearn.metrics.precision_score(y2, classes2, pos_label=1)\n",
    "            score3 = sklearn.metrics.recall_score(y2, classes2, pos_label=1)\n",
    "            score4 = sklearn.metrics.classification_report(y3, classes, digits=4)\n",
    "\n",
    "            print (\"F1-score Task A\",score2,score3,score)\n",
    "            print (\"F1-score Task B\",score4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
